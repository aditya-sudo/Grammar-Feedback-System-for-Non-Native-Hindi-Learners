# ðŸ“ Grammar Feedback System for Non-Native Hindi Learners

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow)](https://huggingface.co/transformers/)
[![Project Repository](https://img.shields.io/badge/GitHub-Project-blue)](https://github.com/aditya-sudo/Grammar-Feedback-System-for-Non-Native-Hindi-Learners)

## ðŸ“š Table of Contents

- [ðŸ“Œ Overview](#-overview)
- [ðŸŽ¯ Objectives](#-objectives)
- [ðŸ§  Key Features](#-key-features)
- [ðŸ“Š Results](#-results)
- [ðŸ“š Learning Outcomes](#-learning-outcomes)
- [ðŸ‘¨â€ðŸ’» Authors](#-authors)

---

## ðŸ“Œ Overview

This project explores sentence-level grammatical error correction (GEC) in **Hindi**, a low-resource and morphologically rich language. The system targets personalized feedback for Hindi learners by leveraging advanced NLP models trained on both synthetic and real-world datasets.

Key models such as **T5**, **MarianMT**, **DistilBERT**, and **BERT** were used for detection, classification, and correction tasks. Performance was evaluated using BLEU/GLEU scores and human assessments.

## ðŸŽ¯ Objectives

- Build a personalized Hindi grammar correction system for non-native learners.
- Generate and curate datasets using Wikipedia edit histories and synthetic rule-based errors.
- Compare token-level and sentence-level GEC approaches using transformer models.
- Fine-tune state-of-the-art models for accurate sentence-level grammatical correction.

## ðŸ§  Key Features

- **Data Creation**: 5 custom datasets including:
  - Synthetic errors (inflectional, word order, number agreement, case markers)
  - Real Hindi Wikipedia edit-based corpus (HiWikEd)
  - Back-translated Hindi sentences using Helsinki-NLP models

- **Dataset Process**:
  - Used Hindi Wikipedia dump (June 2024) and WikiExtractor to extract clean sentences.
  - Generated synthetic errors using rule-based POS tagging (via Stanza) and random perturbations.
  - Extracted real edits using a modified WikiEdits script to build the HiWikEd corpus.
  - Augmented data using ordered back-translation with HuggingFace translation pipelines.

- **ðŸ”— Dataset Access**:  
  [ðŸ“‚ Click here to access the dataset on SharePoint](https://gmuedu-my.sharepoint.com/:f:/g/personal/uyalaman_gmu_edu/El9MKJSpVBtGutpSgO0OHxAB6IN_FFDzzj_mI7uipYpyLQ?e=dHs31D)

- **Model Pipeline**:
  - `DistilBERT`: Sentence correctness classification
  - `BERT`: Token-level error labeling (limited success)
  - `MarianMT` and `T5`: Sentence-level correction (fine-tuned for Hindi)

- **Training Infrastructure**: Fine-tuned on A100 80GB GPU with gradient accumulation and mixed-precision training.

- **Evaluation**:
  - Fine-tuned MarianMT achieved **+9.6% BLEU** and **+13.9% GLEU** over the baseline.
  - Example Correction:
    - âŒ *"à¤‰à¤¸à¤•à¥€ à¤ªà¥à¤°à¤¤à¤¿à¤­à¤¾ à¤•à¥€ à¤—à¤¹à¤°à¤¾à¤ˆ à¤•à¤¿à¤¸à¥€ à¤…à¤¨à¤œà¤¾à¤¨à¥‡ à¤¸à¤®à¥à¤¦à¥à¤° à¤œà¥ˆà¤¸à¤¾ à¤¹à¥ˆ"*
    - âœ… *"à¤‰à¤¸à¤•à¥€ à¤ªà¥à¤°à¤¤à¤¿à¤­à¤¾ à¤•à¥€ à¤—à¤¹à¤°à¤¾à¤ˆ à¤•à¤¿à¤¸à¥€ à¤…à¤¨à¤œà¤¾à¤¨à¥‡ à¤¸à¤®à¥à¤¦à¥à¤° à¤œà¥ˆà¤¸à¥€ à¤¹à¥ˆ"*

## ðŸ“Š Results

| Model               | BLEU | GLEU |
|--------------------|------|------|
| MarianMT (baseline)| 0.52 | 0.43 |
| MarianMT (tuned)   | 0.57 | 0.49 |

| Model               | Precision | Recall | F1-Score |
|--------------------|-----------|--------|----------|
| DistilBERT (baseline) | 0.48   | 0.36   | 0.41     |
| DistilBERT (tuned)    | 0.68   | 0.58   | 0.62     |

## ðŸ“š Learning Outcomes

- Gained practical experience in **low-resource NLP**, **dataset creation**, and **transformer fine-tuning**.
- Discovered the limitations of token-level GEC for Hindi and transitioned to sentence-level correction.
- Conducted in-depth error analysis to evaluate model performance on idioms, compound sentences, and inflections.

## ðŸ‘¨â€ðŸ’» Authors

- Aditya Shah â€“ [GitHub](https://github.com/aditya-sudo)
- Nikhil Chukka
- Uddip Yalamanchili

---

## ðŸ—‚ï¸ Code Structure

The repository includes:

```
ðŸ“¦Grammar-Feedback-System-for-Non-Native-Hindi-Learners/
â”œâ”€â”€ ðŸ“ data/                      # Scripts and datasets for synthetic and real error generation
â”‚   â”œâ”€â”€ hiwikied_extraction.py   # Extracts corrections from Hindi Wikipedia edits
â”‚   â”œâ”€â”€ generate_synthetic.py    # Adds inflectional, agreement, word order errors
â”‚   â””â”€â”€ back_translation.py      # Ordered back-translation using Helsinki-NLP
â”‚
â”œâ”€â”€ ðŸ“ models/                   # Model training scripts
â”‚   â”œâ”€â”€ train_distilbert.py      # Sentence-level classification with DistilBERT
â”‚   â”œâ”€â”€ train_t5.py              # Sentence correction using T5
â”‚   â”œâ”€â”€ train_marianmt.py        # Sentence correction using MarianMT
â”‚   â””â”€â”€ evaluate.py              # Evaluation script (BLEU, GLEU, precision, recall)
â”‚
â”œâ”€â”€ ðŸ“ utils/                    # Utilities
â”‚   â”œâ”€â”€ tokenizer_utils.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ ðŸ“„ report.pdf                # Final report with methodology and results
â”œâ”€â”€ ðŸ“„ README.md                 # Project overview
â””â”€â”€ ðŸ“„ requirements.txt          # Python dependencies
```

To reproduce results:

```bash
# Create and activate virtual environment
python3 -m venv env
source env/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run data preprocessing
python data/hiwikied_extraction.py

# Train models
python models/train_t5.py
python models/train_marianmt.py

# Evaluate results
python models/evaluate.py
```

> For detailed configuration, refer to model-specific scripts under `/models`.

